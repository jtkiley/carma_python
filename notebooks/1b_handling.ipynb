{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling\n",
    "\n",
    "In this notebook, we will work with the following:\n",
    "\n",
    "- Reading data with `pandas`.\n",
    "- Cleaning and transforming data.\n",
    "- Viewing and selecting data.\n",
    "- Merging and querying.\n",
    "- Exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"mode.copy_on_write\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "`pandas` supports a number of formats that we often find ourselves using.\n",
    "For example, I often use data in the Stata `dta` and SAS `sas7bdat` formats.\n",
    "In particular, if you find yourself putting full datasets from WRDS (especially the ones that are not accessible with web forms), you will end up using the SAS format.\n",
    "\n",
    "`pandas` also handles formats like Excel `xlsx`, comma separated values `csv` (and, indeed, nearly any delimited file), and fixed width data.\n",
    "The acquisition database, SDC Platinum, has a somewhat unreliable Excel output feature, and the `pandas` fixed width format reader takes nearly all of the pain out of reading in data exported that way.\n",
    "\n",
    "Note: `pandas` can also write many of the formats that it can read.\n",
    "A notable exception is `sas7bdat` because it is proprietary and undocumented.\n",
    "The reader was written with some clever reverse engineering, but writing a valid file is difficult and probably not coming in the future (see [Github issue](https://github.com/pandas-dev/pandas/issues/13031)).\n",
    "An easy workaround is using the SAS open format `xpt` or `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stata data\n",
    "firmyear = pd.read_stata(\"../data/firmyear.dta\")\n",
    "firmyear.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "You are likely familiar with a number of data cleaning issues.\n",
    "However, you may not yet know how to map on what you know in another program to Python.\n",
    "The pandas documentation has a number of comparison references, including [R](https://pandas.pydata.org/pandas-docs/stable/comparison_with_r.html), [Stata](https://pandas.pydata.org/pandas-docs/stable/comparison_with_stata.html) and [SAS](https://pandas.pydata.org/pandas-docs/stable/comparison_with_sas.html).\n",
    "\n",
    "Some brief examples are below.\n",
    "\n",
    "## Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the columns above are of type `object`, which often means that they are strings.\n",
    "We want to change the things that we know are numbers (i.e. `count_of_employees` and `year`) into the appropriate types (both `int` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear[\"year\"] = firmyear[\"year\"].astype(\"int\")\n",
    "firmyear[\"count_of_employees\"] = firmyear[\"count_of_employees\"].astype(\"int\")\n",
    "\n",
    "# Note, a more general version would be:\n",
    "# cols = firmyear.columns.drop('name')\n",
    "# firmyear[cols] = firmyear[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns\n",
    "\n",
    "We could rename columns by creating a new column with the correct name and dropping the prior one, but this is more efficient and easily extended to the multiple column case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of using dictionaries.\n",
    "COLUMNS = {\"count_of_employees\": \"size_emp\"}\n",
    "\n",
    "firmyear = firmyear.rename(columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "We can also do transformations that apply some sort of function or method to data by groups.\n",
    "This is a fairly simple example, but `pandas` makes it fairly easy to do sophisticated transformations.\n",
    "See the [split-apply-combine](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) documentation.\n",
    "This is a big topic, and, like before, we are only scratching the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do per-group things like calculating differences.\n",
    "firmyear[\"size_emp_change\"] = firmyear.groupby(firmyear[\"name\"])[\"size_emp\"].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing and selecting data\n",
    "\n",
    "pandas has a number of tools for viewing and selecting data.\n",
    "The one we see above is the `df.head()` method that displays the first five rows at the top (or head) of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can give it a parameter to modify the number of rows.\n",
    "# Here, we only have six rows, so that's all we get.\n",
    "firmyear.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The len() function works on dataframes.\n",
    "len(firmyear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select one or more columns by using indexing that is somewhat like what we did with dictionaries earlier.\n",
    "However, we can give the indexer a list, and get the named columns.\n",
    "\n",
    "Note that, when we ask for one column, pandas gives us a series, not a dataframe, so the display is a little less fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the two sets of brackets.\n",
    "# The outer set is for the indexing syntax.\n",
    "# The inner set is for the list that we're asking the indexer for.\n",
    "firmyear[[\"name\", \"year\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for rows that meet certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear[firmyear[\"name\"] == \"Microsoft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the expression used for indexing is returning a series of boolean values.\n",
    "firmyear[\"name\"] == \"Microsoft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use compound statements that return one boolean value per row.\n",
    "# Here, it's name == Microsoft or the year is less than 2018.\n",
    "firmyear[(firmyear[\"name\"] == \"Microsoft\") | (firmyear[\"year\"] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series have methods for checking whether they're NA.\n",
    "# This is true if each row is not NA.\n",
    "firmyear[firmyear[\"size_emp_change\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is True if each row is NA.\n",
    "firmyear[firmyear[\"size_emp_change\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use ~ to negate the condition after it.\n",
    "# So, here is not not NA (same as is NA).\n",
    "firmyear[~firmyear[\"size_emp_change\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n",
    "\n",
    "Like other software, `pandas` is great and merging data, and it as some conveniences not found in most other software.\n",
    "\n",
    "Let's work through a simple example to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our firm year data.\n",
    "firmyear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv(\"../data/stock.csv\")\n",
    "stock.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to do is merge in those Microsoft stock prices from the beginning of those years.\n",
    "It's a bit contrived for an example, but it mirrors a lot of real world work.\n",
    "\n",
    "While we know that Microsoft's ticker is MSFT, there's no way for `pandas` to know that without help.\n",
    "So, to help, we'll make a lookup table using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {\"Microsoft\": \"MSFT\", \"Google\": \"GOOG\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear[\"id_ticker\"] = firmyear[\"name\"].map(lookup)\n",
    "firmyear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make that lowercase.\n",
    "firmyear[\"id_ticker\"] = firmyear[\"id_ticker\"].str.lower()\n",
    "firmyear.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stata, we would have another problem, namely that our column names for merging do not match.\n",
    "With `pandas`, that's not a problem.\n",
    "\n",
    "Note the validate parameter. This tells pandas that we have an expectation about how these data align with each other, and it should raise an exception if our expectation isn't met.\n",
    "If you merge data without this parameter, and it unexpectedly grows in length, you may be unintentionally doing a many-to-many merge (which generally returns a new row for every pair of matches within the groups specified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear = firmyear.merge(\n",
    "    stock,\n",
    "    how=\"left\",\n",
    "    left_on=[\"id_ticker\", \"year\"],\n",
    "    right_on=[\"tic\", \"yr\"],\n",
    "    validate=\"1:1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying\n",
    "\n",
    "When working with content data, we often need to do some sort of a query to aggregate data that is interesting to us.\n",
    "\n",
    "For example, let's add the an average word count of articles from some NYT data (similar to what we'll retrieve later) to our firmyear data.\n",
    "We're only going to have results for 2018, as that's all the data I included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_nyt = pd.read_csv(\"../data/msft_nyt.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_nyt[\"pub_date\"] = pd.to_datetime(msft_nyt[\"pub_date\"])\n",
    "msft_nyt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_AGG = {\"word_count\": [\"mean\", \"sum\"]}\n",
    "\n",
    "\n",
    "def query_docs(data, ticker, year):\n",
    "    summary = (\n",
    "        data[(data[\"id_ticker\"] == ticker) & (data[\"pub_date\"].dt.year == year)]\n",
    "        .agg(_AGG)\n",
    "        .T.reset_index(drop=True)\n",
    "    )\n",
    "    summary[\"id_ticker\"] = ticker\n",
    "    summary[\"year\"] = year\n",
    "    summary = summary.rename(columns={\"mean\": \"wc_mean\", \"sum\": \"wc_sum\"})\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for index, row in firmyear[[\"id_ticker\", \"year\"]].iterrows():\n",
    "    result_list.append(query_docs(msft_nyt, row[\"id_ticker\"], row[\"year\"]))\n",
    "results = pd.concat(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the same operation condensed into a list comprehension.\n",
    "# results = pd.concat([query_docs(msft_nyt, row['id_ticker'], row['year'])\n",
    "#                      for i, row in firmyear[['id_ticker', 'year']].iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear = firmyear.merge(results, how=\"left\", on=[\"id_ticker\", \"year\"], validate=\"1:1\")\n",
    "firmyear.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and exporting\n",
    "\n",
    "pandas is able to write data in a number of formats that you may need.\n",
    "You can see a [reference](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) in the user guide.\n",
    "\n",
    "Two in particular merit an additional mention:\n",
    "\n",
    "1. **Parquet.** Apache Parquet is a high-performance compressed file format that I like to use for data that I want to use again in Python. It retains the type information, and it continues to work well up to file sizes of a few GBs.\n",
    "1. **SQL.** If you are working with a database directly (including the ones we will see later), the SQL support in pandas is really convenient. That said, if you are using a service with its own package (e.g., WRDS), you probably want the more specific package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout Exercises\n",
    "\n",
    "Time permitting, try out some of the data handing techniques that we learned above on your own data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX1: try your data\n",
    "\n",
    "Let's use pandas on a dataset you already have.\n",
    "\n",
    "1. Read your dataset into a pandas dataframe with the name `my_data`. To find the proper function, you may want to look at the [pandas IO reference](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
    "1. Display the first 10 rows.\n",
    "1. Display the datatypes of the columns. Notice any problems.\n",
    "1. Try some of the skills we learned above. For example, you might rename a column or select the data where a certain column takes a value (or satisfies some condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4 code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus content\n",
    "\n",
    "One thing to notice in our code above is that we have several datasets all in memory at once.\n",
    "In some stats packages, this is not nearly so easy.\n",
    "\n",
    "For example, in Stata, they recently added the concept of multiple datasets, but the interface is much more difficult to use.\n",
    "In contrast, with pandas, we simply use the name of the dataframe and then whatever operation that we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firmyear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_nyt.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

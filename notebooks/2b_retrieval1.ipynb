{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval I\n",
    "\n",
    "In this notebook, we will work with the following:\n",
    "\n",
    "- Web scraping process.\n",
    "- Read one page.\n",
    "- Find the content we want.\n",
    "- Automate many pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "One helpful way of gathering text data is web scraping.\n",
    "We usually do this in three steps:\n",
    "\n",
    "1. Retrieve the pages with information we want.\n",
    "1. Extract the data from the pages.\n",
    "1. Clean and save the resulting data.\n",
    "\n",
    "Let's walk through an example of getting press releases from the [Microsoft website](https://news.microsoft.com/category/press-releases/).\n",
    "\n",
    "I often prefer to work out of order as follows:\n",
    "\n",
    "1. Figure out how to extract data from one page that has the data.\n",
    "1. Then, figure out how to automate getting the pages of interest.\n",
    "1. Run those pages through the procedure in step 1.\n",
    "1. Clean and save.\n",
    "\n",
    "This has the benefit of solving what is usually the hardest problem first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "\n",
    "As you'll see, the difficulty ramps up a lot here.\n",
    "Web scraping is easily a full day topic on its own.\n",
    "Hence, I have two main goals for you:\n",
    "\n",
    "1. Get a sense of the logic and the process in solving the problem. This is a good start if you want to learn it yourself.\n",
    "1. Understand what is feasible and achievable. This helps whether you do it yourself or farm it out (and there's a ready talent pool for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read one page\n",
    "\n",
    "This is the hardest part.\n",
    "\n",
    "Note that we add a user agent header that is sent as part of the request.\n",
    "The reason is that a lot of web servers block user agents that are web scraping tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_AGENT = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\"\n",
    "\n",
    "pr_url_1 = (\n",
    "    \"https://news.microsoft.com/2018/10/04/\"\n",
    "    \"redline-communications-and-microsoft-announce-\"\n",
    "    \"partnership-to-lower-the-cost-of-tv-white-space-solutions/\"\n",
    ")\n",
    "pr_req_1 = requests.get(pr_url_1, headers={\"User-Agent\": _AGENT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be 200, which is the code for OK.\n",
    "pr_req_1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .text attribute of the request object is the HTML of the page.\n",
    "pr_soup_1 = BeautifulSoup(pr_req_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta tags have some data we'd like to get.\n",
    "# For example, this is the published time.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the property attribute of this meta tag,\n",
    "# which has the name of the data item.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"property\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content attribute has the data item itself.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of meta tags to get.\n",
    "# Note: when in doubt, get everything you might possibly use.\n",
    "#       It's easier to drop stuff than to re-scrape everything.\n",
    "\n",
    "_METAS = [\n",
    "    \"article:published_time\",\n",
    "    \"article:modified_time\",\n",
    "    \"og:title\",\n",
    "    \"og:description\",\n",
    "    \"og:updated_time\",\n",
    "    \"og:url\",\n",
    "    \"article:section\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop populates a dict with each of the meta attributes above and its content.\n",
    "# Discussion: why is this try/except necessary? What happens if we remove it?\n",
    "pr_data_1 = {}\n",
    "for meta in _METAS:\n",
    "    try:\n",
    "        prop = pr_soup_1.find(\"meta\", property=meta)[\"property\"]\n",
    "        content = pr_soup_1.find(\"meta\", property=meta)[\"content\"]\n",
    "    except TypeError:\n",
    "        prop = meta\n",
    "        content = \"\"\n",
    "    pr_data_1.update({prop: content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find(\"h3\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1[\"h3\"] = (\n",
    "    pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find(\"h3\").text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_soup_1.find(\"div\", {\"class\": \"entry-content m-blog-content\"}).find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little gnarly.\n",
    "pr_data_1[\"body\"] = \"\\n\\n\".join(\n",
    "    [\n",
    "        i.text\n",
    "        for i in pr_soup_1.find(\n",
    "            \"div\", {\"class\": \"entry-content m-blog-content\"}\n",
    "        ).find_all(\"p\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate our one page work.\n",
    "\n",
    "This is fairly easy. We have the code for it already.\n",
    "We just need to wrap it in a function.\n",
    "\n",
    "**Note:** I'm using an `if` statement to check whether these properties exist, and guarding against the case where they don't.\n",
    "I did this iteratively while building this content, because I noticed (from errors) that many press releases do not have modification dates or article sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_soup(soup):\n",
    "    data = {}\n",
    "    for meta in _METAS:\n",
    "        if soup.find(\"meta\", property=meta) is not None:\n",
    "            prop = soup.find(\"meta\", property=meta)[\"property\"]\n",
    "        if soup.find(\"meta\", property=meta) is not None:\n",
    "            content = soup.find(\"meta\", property=meta)[\"content\"]\n",
    "        if prop is not None and content is not None:\n",
    "            data.update({prop: content})\n",
    "    try:\n",
    "        data[\"h3\"] = (\n",
    "            soup.find(\"div\", {\"class\": \"entry-content m-blog-content\"})\n",
    "            .find(\"h3\")\n",
    "            .string\n",
    "        )\n",
    "    except AttributeError:\n",
    "        data[\"h3\"] = \"\"\n",
    "\n",
    "    data[\"body\"] = \"\\n\\n\".join(\n",
    "        [\n",
    "            i.text\n",
    "            for i in soup.find(\n",
    "                \"div\", {\"class\": \"entry-content m-blog-content\"}\n",
    "            ).find_all(\"p\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how easy this is once we make a function.\n",
    "get_data_from_soup(pr_soup_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read many pages\n",
    "\n",
    "Now we need to get the URLs for all of the pages we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_url_1 = \"https://news.microsoft.com/category/press-releases/\"\n",
    "many_pr_page_1 = requests.get(many_pr_url_1, headers={\"User-Agent\": _AGENT}).text\n",
    "many_pr_soup_1 = BeautifulSoup(many_pr_page_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost, but note the ones at the bottom.\n",
    "many_pr_soup_1.find(\"section\", id=\"primary\").find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we further filter down to articles and then get their hrefs to\n",
    "#    eliminate the navigation links at the bottom.\n",
    "articles = many_pr_soup_1.find(\"section\", id=\"primary\").find_all(\"article\")\n",
    "links = [i.find(\"a\")[\"href\"] for i in articles]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_links_1 = links.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate getting links and data from each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to turn links into soup objects a lot, so let's make a function.\n",
    "def link_to_soup(link):\n",
    "    page = requests.get(link, headers={\"User-Agent\": _AGENT}).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_links_from_link_page(link_page):\n",
    "    soup = link_to_soup(link_page)\n",
    "    articles = soup.find(\"section\", id=\"primary\").find_all(\"article\")\n",
    "    links = [i.find(\"a\")[\"href\"] for i in articles]\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_data_from_links(links):\n",
    "    data_list = []\n",
    "    for link in links:\n",
    "        soup = link_to_soup(link)\n",
    "        data_list.append(get_data_from_soup(soup))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_prs = pd.DataFrame(get_data_from_links(many_pr_links_1))\n",
    "msft_prs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further automation\n",
    "\n",
    "**Note**: for running time reasons, we're not going to make a multi-links-page version, but note that there's a next page link at the bottom of those pages that can be extracted to build that:\n",
    "\n",
    "```html\n",
    "<a href=\"/category/press-releases/page/2/?paged=3\" \n",
    "   class=\"c-glyph x-hidden-focus\" \n",
    "   aria-label=\"Go to next page\" ms.title=\"Next Page\">\n",
    "```\n",
    "\n",
    "However, we could also notice that the link pages have a number in the URL that is incremented by one for each page.\n",
    "We would have to look at a page to get the end number, but we could also simply use a loop to construct a URL for each of those numbers.\n",
    "\n",
    "`https://news.microsoft.com/category/press-releases/page/2/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df6ffe3",
   "metadata": {},
   "source": [
    "# Data retrieval I\n",
    "\n",
    "In this notebook, we will work with the following:\n",
    "\n",
    "- Web scraping process.\n",
    "- Read one page.\n",
    "- Find the content we want.\n",
    "- Automate many pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6efd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"mode.copy_on_write\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43414c",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "One helpful way of gathering text data is web scraping.\n",
    "We usually do this in three steps:\n",
    "\n",
    "1. Retrieve the pages with information we want.\n",
    "1. Extract the data from the pages.\n",
    "1. Clean and save the resulting data.\n",
    "\n",
    "Let's walk through an example of getting press releases from the [Alphabet website](https://abc.xyz/investor/news/2024/).\n",
    "\n",
    "I often prefer to work out of order as follows:\n",
    "\n",
    "1. Figure out how to extract data from one page that has the data.\n",
    "1. Then, figure out how to automate getting the pages of interest.\n",
    "1. Run those pages through the procedure in step 1.\n",
    "1. Clean and save.\n",
    "\n",
    "This has the benefit of solving what is usually the hardest problem first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d7d60",
   "metadata": {},
   "source": [
    "## Important note\n",
    "\n",
    "As you'll see, the difficulty ramps up a lot here.\n",
    "Web scraping is easily a full day topic on its own.\n",
    "Hence, I have two main goals for you:\n",
    "\n",
    "1. Get a sense of the logic and the process in solving the problem. This is a good start if you want to learn it yourself.\n",
    "1. Understand what is feasible and achievable. This helps whether you do it yourself or farm it out (and there's a ready talent pool for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2d029",
   "metadata": {},
   "source": [
    "## Read one page\n",
    "\n",
    "This is the hardest part.\n",
    "\n",
    "Note that we add a user agent header that is sent as part of the request.\n",
    "The reason is that a lot of web servers block user agents that are web scraping tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d40f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "    \" (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.3\"\n",
    ")\n",
    "\n",
    "pr_url_1 = \"https://abc.xyz/2024-1010/\"\n",
    "\n",
    "pr_req_1 = requests.get(pr_url_1, headers={\"User-Agent\": AGENT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this to be 200, which is the code for OK.\n",
    "pr_req_1.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d636e",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "This is a very deep topic that we only need to barely touch.\n",
    "In short, there are many standards for representing text as mappings of bytes (eight 0 or 1 values).\n",
    "Many of them have significant overlap (based on underlying standards that they are a superset of), such that they at least mostly work, but it's better if we're sure we're using the right encoding.\n",
    "\n",
    "In our example here, the server sends data in such a way that we would infer that the text is in the `ISO-8859-1` encoding, though it is actually in the `UTF-8` encoding.\n",
    "Fortunately, `requests` can tell us both what the encoding is and what it thinks it actually is, so we can build upon that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceca4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_req_1.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_req_1.apparent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_req_1.encoding = pr_req_1.apparent_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed700fe",
   "metadata": {},
   "source": [
    "### Extracting content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42895b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .text attribute of the request object is the HTML of the page.\n",
    "pr_soup_1 = BeautifulSoup(pr_req_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta tags have some data we'd like to get.\n",
    "# For example, this is the published time.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946eeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the property attribute of this meta tag,\n",
    "# which has the name of the data item.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"property\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa825aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content attribute has the data item itself.\n",
    "pr_soup_1.find(\"meta\", property=\"article:published_time\")[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of meta tags to get.\n",
    "# Note: when in doubt, get everything you might possibly use.\n",
    "#       It's easier to drop stuff than to re-scrape everything.\n",
    "\n",
    "METAS = [\n",
    "    \"article:published_time\",\n",
    "    \"article:modified_time\",\n",
    "    \"og:title\",\n",
    "    \"og:description\",\n",
    "    \"og:updated_time\",\n",
    "    \"og:url\",\n",
    "    \"article:section\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38187cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop populates a dict with each of the meta attributes above and its content.\n",
    "# Discussion: why is this try/except necessary? What happens if we remove it?\n",
    "pr_data_1 = {}\n",
    "for meta in METAS:\n",
    "    try:\n",
    "        prop = pr_soup_1.find(\"meta\", property=meta)[\"property\"]\n",
    "        content = pr_soup_1.find(\"meta\", property=meta)[\"content\"]\n",
    "    except TypeError:\n",
    "        prop = meta\n",
    "        content = \"\"\n",
    "    pr_data_1.update({prop: content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_soup_1.find(\"div\", {\"class\": \"RichTextArticleBody RichTextBody\"}).find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little gnarly.\n",
    "pr_data_1[\"body\"] = \"\\n\\n\".join(\n",
    "    [\n",
    "        i.text\n",
    "        for i in pr_soup_1.find(\n",
    "            \"div\", {\"class\": \"RichTextArticleBody RichTextBody\"}\n",
    "        ).find_all(\"p\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea75bb",
   "metadata": {},
   "source": [
    "# Automate our one page work.\n",
    "\n",
    "This is fairly easy. We have the code for it already.\n",
    "We just need to wrap it in a function.\n",
    "\n",
    "**Note:** I'm using an `if` statement to check whether these properties exist, and guarding against the case where they don't.\n",
    "I did this iteratively while building this content, because I noticed (from errors) that many press releases do not have modification dates or article sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_soup(soup):\n",
    "    data = {}\n",
    "    for meta in METAS:\n",
    "        try:\n",
    "            prop = soup.find(\"meta\", property=meta)[\"property\"]\n",
    "            content = soup.find(\"meta\", property=meta)[\"content\"]\n",
    "        except TypeError:\n",
    "            prop = meta\n",
    "            content = \"\"\n",
    "        data.update({prop: content})\n",
    "\n",
    "    data[\"body\"] = \"\\n\\n\".join(\n",
    "        [\n",
    "            i.text\n",
    "            for i in soup.find(\n",
    "                \"div\", {\"class\": \"RichTextArticleBody RichTextBody\"}\n",
    "            ).find_all(\"p\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how easy this is once we make a function.\n",
    "get_data_from_soup(pr_soup_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe17a0",
   "metadata": {},
   "source": [
    "## Read many pages\n",
    "\n",
    "Now we need to get the URLs for all of the pages we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7abc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_url_1 = \"https://abc.xyz/investor/news/2024/\"\n",
    "many_pr_page_1 = requests.get(many_pr_url_1, headers={\"User-Agent\": AGENT}).text\n",
    "many_pr_soup_1 = BeautifulSoup(many_pr_page_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ba003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we find the div containing the listings and then find the links within.\n",
    "many_pr_soup_1.find(\"div\", {\"class\": \"PageListW-items\"}).find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, for each of the anchor tags, we can extract the links themselves.\n",
    "articles = many_pr_soup_1.find(\"div\", {\"class\": \"PageListW-items\"}).find_all(\"a\")\n",
    "links = [i[\"href\"] for i in articles]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_pr_links_1 = links.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91b19d",
   "metadata": {},
   "source": [
    "## Automate getting links and data from each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to turn links into soup objects a lot, so let's make a function.\n",
    "def link_to_soup(link):\n",
    "    page_request = requests.get(link, headers={\"User-Agent\": AGENT})\n",
    "    page_request.encoding = page_request.apparent_encoding\n",
    "    page = page_request.text\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_links_from_link_page(link_page):\n",
    "    soup = link_to_soup(link_page)\n",
    "    articles = soup.find(\"div\", {\"class\": \"PageListW-items\"})\n",
    "    links = [i[\"href\"] for i in articles]\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_data_from_links(links):\n",
    "    data_list = []\n",
    "    for link in links:\n",
    "        soup = link_to_soup(link)\n",
    "        data_list.append(get_data_from_soup(soup))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02583472",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_prs = pd.DataFrame(get_data_from_links(many_pr_links_1))\n",
    "alphabet_prs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a49d5a",
   "metadata": {},
   "source": [
    "# Further automation\n",
    "\n",
    "**Note**: for running time reasons, we're not going to make a multi-links-page version, but note that there are year links on the left of the listing pages that can be extracted.\n",
    "\n",
    "However, we could also notice that the link pages have a year in the URL.\n",
    "We would have to look at a page to get the earliest year, but we could otherwise simply use a loop to construct a URL for each of those years.\n",
    "\n",
    "`https://abc.xyz/investor/news/2023/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
